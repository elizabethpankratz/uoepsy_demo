---
title: Detailed analyses
format:
  html:
    code-tools: true
---

```{r setup, include = FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)
library(MASS)

# To solve some conflicts between packages
select <- dplyr::select

# knitr settings
knitr::opts_chunk$set(
  fig.retina=3,
  fig.align = "center",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
knitr::opts_knit$set(root.dir = here::here())

# dplyr and ggplot settings
options(dplyr.summarise.inform = FALSE)
theme_set(theme_classic())
theme_update(
  text = element_text(family = "Fira Sans", size = 16),
  strip.background = element_blank()
)

palette_gender <- c( '#882e72', '#1965b0', '#4eb265')
```


On this page, I lay out in more technical detail [the analysis of the student anxiety data using `MASS::polr()`](#ordinal-regression-with-polr).

I also (very) briefly illustrate two other widespread ways of analysing ordinal data: [treating it as continuous numeric data and using a Gaussian model](#gaussian-regression-model-with-lm), and [first converting it to z-scores by participant and then using a Gaussian model on those z-scores](#z-score-conversion-then-gaussian-regression-model-with-lm).


# The data

The student anxiety rating data comes from [the SMARVUS dataset](https://openpsychologydata.metajnl.com/articles/10.5334/jopd.80) compiled by Terry et al. (2023).^[Terry, J., Ross, R. M., Nagy, T., Salgado, M., Garrido-VÃ¡squez, P., Sarfo, J. O., Cooper, S., Buttner, A. C., Lima, T. J. S., Ã–ztÃ¼rk, Ä°., Akay, N., Santos, F. H., Artemenko, C., Copping, L. T., Elsherif, M. M., MilovanoviÄ‡, I., Cribbie, R. A., Drushlyak, M. G., Swainston, K., â€¦ Field, A. P. (2023). [Data from an International Multi-Centre Study of Statistics and Mathematics Anxieties and Related Variables in University Students (the SMARVUS Dataset).](https://openpsychologydata.metajnl.com/articles/10.5334/jopd.80) *Journal of Open Psychology Data*, 11(1), 8.]

I first filtered the data to keep only participants who responded correctly to all seven attention checks in the original survey.

For this analysis, I'm using responses to question `Q7.1_3`, which asked students to rate their anxiety from 1 (no anxiety) to 5 (a great deal of anxiety) in response to the following scenario: **"Going to ask my statistics teacher for individual help with material I am having difficulty understanding".**

I only analysed responses from participants who passed all seven attention checks in the original survey, and I extracted each participant's unique ID, their self-reported gender (grouped by the researchers into `Female/Woman`, `Male/Man`, and `Another Gender`), and their rating for question `Q7.1_3`.

This data is saved in `anx.csv`.

```{r}
anx <- read_csv('data/anx.csv') |>
  select(unique_id, gender, score) |> 
  rename(rating = score)

slice(anx, 45:50)
```


# Ordinal regression with `polr()`

## Intercept-only model

### Look at the data

Here's how participant responses, aggregated over `gender`, pattern:

```{r}
anx |>
  ggplot(aes(x = factor(rating))) +
  geom_bar(fill = '#2e3836') +
  labs(
    x = element_blank(),
    y = 'Count',
    caption = 'n = 8,314'
  ) +
  scale_x_discrete(labels = c('1\n(no anxiety)', '2', '3', '4', '5\n(a great deal\nof anxiety)')) +
  NULL
```


### Fit the model

To use `polr()`, the outcome variable must be a factor.
No need to manually specify the order of levels, since they're already in the correct order by default.

```{r}
anx <- anx |>
  mutate(rating = factor(rating))

anx_fit1 <- polr(
  rating ~ 1,
  data = anx,
  method = 'probit',
  Hess = TRUE  # set TRUE if want to use summary()
)

summary(anx_fit1)
```

### Interpret the model

The values estimated for the `Intercepts` aren't really intercepts, not like the intercept of a linear model.
Rather, these values represent the thresholds (also called cutpoints) along a continuous latent variable, a construct which the model assumes to represent some continuous measure of anxiety.
Sometimes this assumed continuous latent variable will be represented as $\Phi$ (phi), and sometimes those thresholds will be represented as $\zeta$ (zeta).

We can extract those threshold values as follows:

```{r}
anx_fit1$zeta
```


Because I've used the probit link function (in `method = "probit"`), the model further assumes that anxiety follows a standard normal distribution over this continuous latent variable.^[
If I had used `method = "logit"`, then the model would assume not a normal distribution but a logistic distribution.
Similar shape, but different scale, so (back-)transformation between the model space and probability space would happen not with `pnorm()`/`qnorm()` but with `plogis()`/`qlogis()`.
]


```{r}
rating_labs1 <- tibble(
  x = c(-2.6, -1.2, -0.5, 0.1, 0.7, 1.3),
  d = c(0.47),
  labs = c('Rating:', 1:5)
)

p_underlying_normal <- tibble(x = seq(from = -3.5, to = 3.5, length.out = 200)) |> 
  mutate(d = dnorm(x = x)) |> 
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = '#2e3836', alpha = 0.4) +
  geom_text(data = rating_labs1, aes(label = labs), family = "Fira Sans", size = 8) +
  geom_vline(xintercept = anx_fit1$zeta, linetype = 'dotted', linewidth = 1) +
  scale_x_continuous(
    'Anxiety (assumed latent variable)',
    breaks = -3:3,
    sec.axis = dup_axis(
      name = NULL,
      breaks = anx_fit1$zeta |> as.double(),
      labels = round(anx_fit1$zeta, 2)
    )
  ) +
  scale_y_continuous(
    NULL, 
    breaks = NULL,
    limits = c(0, 0.5)
    ) +
  theme( 
    axis.line.x.top = element_blank(),
    axis.ticks.x.top = element_blank()
  ) +
  NULL

p_underlying_normal
```

This normal distribution allows us to compute the probability density within each region defined by the cutpoints.
One way to do this is using `pnorm()`, which gives us by default how much probability density falls to the left of the given value in a standard normal distribution.
For the intermediate regions, we subtract the probability density below the region's leftward periphery from the probability density below its rightward periphery, to get only the density between those two bounds.

```{r}
(anx_fit1_p1 <- pnorm(-0.84))                  # probability of rating 1
(anx_fit1_p2 <- pnorm(-0.17) - pnorm(-0.84))   # probability of rating 2
(anx_fit1_p3 <- pnorm(0.38) - pnorm(-0.17))    # probability of rating 3
(anx_fit1_p4 <- pnorm(1.03) - pnorm(0.38))     # probability of rating 4
(anx_fit1_p5 <- pnorm(1000) - pnorm(1.03))     # probability of rating 5
```
(For rating 5, I just grabbed some huge value as an arbitrary upper bound.)

A good sense check: these probabilities approximate the overall proportion of responses to each rating in the original data.

```{r}
table(anx$rating) / nrow(anx)
```


Here's one way to visualise the probability density within each region of the distribution.

```{r}
anx_fit1_probs <- c(anx_fit1_p1, anx_fit1_p2, anx_fit1_p3, anx_fit1_p4, anx_fit1_p5)

prob_labs1 <- tibble(
  x = c(-2.6, -1.2, -0.5, 0.1, 0.7, 1.3),
  d = c(0.1),
  labs = c('Probability:', round(anx_fit1_probs, 2))
)

p_underlying_normal +
  geom_text(data = prob_labs1, aes(label = labs), family = "Fira Sans")
```



## `rating ~ gender`

### Look at the data

I'll first convert `gender` to a factor with a custom ordering.

```{r}
anx <- anx |>
  mutate(gender = factor(gender, levels = c('Female/Woman', 'Male/Man', 'Another Gender')))
```

Participants' anxiety ratings for the same question as above, now divided by their gender:

```{r, fig.height=6}
anx |>
  ggplot(aes(x = rating, fill = gender)) +
  geom_bar() +
  facet_wrap(~ gender, scales = 'free', nrow=3) +
  scale_fill_manual(values = palette_gender) +
  labs(
    x = element_blank(),
    y = 'Count'
  ) +
  theme(legend.position = 'none') +
  scale_x_discrete(labels = c('1\n(no anxiety)', '2', '3', '4', '5\n(a great deal\nof anxiety)')) +
  NULL
```

(Yikes, right? ðŸ˜ž)


### Fit the model

I'm going to use R's default treatment coding for `gender`.
Because of the level ordering I specified above for `gender`, `Female/Woman` will be the reference level, and we'll get two dummy variables: one comparing `Female/Woman` to `Male/Man`, and one comparing `Female/Woman` to `Another Gender`.
To confirm:

```{r}
contrasts(anx$gender)
```

```{r}
anx_fit2 <- polr(
  rating ~ gender,
  data = anx,
  method = 'probit',
  Hess = TRUE
)

summary(anx_fit2)
```

### Interpret the model

Just as the `Intercepts` aren't the intercept of a line, the `Coefficients` are not the slope of a line.
Rather, they give the mean of the normal distribution that represents how each gender group's ratings are distributed over the continuous latent variable representing anxiety.

In other words, the normal distribution representing `Male/Man` anxiety is shifted leftward by 0.33 units from the reference distribution (`Female/Woman`, still centered at 0), while the `Another Gender` distribution is shifted rightward by 0.48 units.

```{r}
round(coef(anx_fit2), 2)
```


The following code computes the probability density at each value of this latent continuous variable for each gender group's distribution.
I'll use it to plot all three groups' probability distributions below.

```{r}
anx_fit2_latent_normals <- tibble(
  Gender = factor(
    c('Female/Woman', 'Male/Man', 'Another Gender'),
    levels = c('Female/Woman', 'Male/Man', 'Another Gender')),
  mu = c(0, coef(anx_fit2))
) |>
  tidyr::expand(
    nesting(Gender, mu),
    x = seq(from = -3.5, to = 3.5, length.out = 200)
  ) |>
  mutate(
    d = dnorm(x, mean = mu, sd = 1)  # get dens at each x value for normal distrib with each row's mu
  )

head(anx_fit2_latent_normals)
```




```{r fig.height = 6}
rating_labs <- tibble(
  x = c(-2.6, -1.4, -0.6, 0.1, 0.7, 1.5),
  d = c(0.57),
  labs = c('Rating:', 1:5),
  Gender = c('Female/Woman')
)

mm_mean <- anx_fit2$coefficients[['genderMale/Man']]
ag_mean <- anx_fit2$coefficients[['genderAnother Gender']]

anx_fit2_latent_normals |>
  ggplot(aes(x = x, y = d, fill = Gender)) +
  geom_area(position = 'identity', alpha = 0.5) +
  geom_vline(xintercept = anx_fit2$zeta, linetype = 'dotted', linewidth = 1) +
  geom_vline(linewidth = 1, xintercept = 0, colour = palette_gender[1]) +
  geom_vline(linewidth = 1, xintercept = mm_mean, colour = palette_gender[2]) +
  geom_vline(linewidth = 1, xintercept = ag_mean, colour = palette_gender[3]) +
  geom_text(data = rating_labs, aes(label = labs), family = 'Fira Sans', size = 8) +
  scale_fill_manual(values = palette_gender) +
  theme(
    axis.line.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    legend.position = 'bottom'
  ) +
  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 0.6)) +
  scale_x_continuous(
    'Anxiety (assumed latent variable)', 
    breaks = -3:3,
    sec.axis = dup_axis(
      name = NULL,
      breaks = anx_fit2$zeta |> as.double(),
      labels = round(anx_fit2$zeta, 2)
    )) +
  geom_segment(aes(x= 0, y = 0.44, xend = mm_mean, yend = 0.44), linewidth = 1.5, arrow = arrow(length = unit(0.5,"cm")), colour = palette_gender[2]) +
  geom_label(aes(x = -0.33, y = 0.49), label = round(mm_mean, 2), fill = 'white', colour = palette_gender[2], family = 'Fira Sans', label.size = 1, size = 6) +
  geom_segment(aes(x= 0, y = 0.44, xend = ag_mean, yend = 0.44), linewidth = 1.5, arrow = arrow(length = unit(0.5,"cm")), colour = palette_gender[3]) +
  geom_label(aes(x = 0.48, y = 0.49), label = round(ag_mean, 2), fill = 'white', colour = palette_gender[3], family = 'Fira Sans', label.size = 1, size = 6) +
  NULL
```


Shifting the normal distributions left and right changes how much probability density is allocated to each region, each rating.
The thresholds do not move.

The following function estimates the probability density between given thresholds for a normal distribution with given mean and standard deviation (in these models, I've allowed the standard deviation to stay fixed at 1).

```{r}
get_prob_dens <- function(zetas, mu, sigma){
  # zetas: numeric vector, list of thresholds in Phi space (from polr model)
  # mu: numeric, mean of normal distribution
  # sigma: positive numeric, standard deviation of normal distribution
  
  # Back-transform thresholds into probability space according to location 
  # of shifted normal. Outcome contains cumulative probabilities.
  prob_spc_thresholds <- pnorm(zetas, mean = mu, sd = sigma)  
  
  # Add first and last elements to this cumsum so that successive differences 
  # come out right.
  probs_btwn_thresholds <- diff( c(`1` = 0, prob_spc_thresholds, 'x' = 1) )
  
  # Rename each element in the vector to the between-threshold sections.
  names(probs_btwn_thresholds) <- 1:length(probs_btwn_thresholds)
  
  probs_btwn_thresholds
}
```

Another way of visualising each resulting probability distributions is as follows.

```{r}
fw_probs <- get_prob_dens(anx_fit2$zeta, 0, 1)

fw_prob_labs <- tibble(
  x = c(-2.6, -1.4, -0.56, 0.05, 0.66, 1.45),
  d = c(0.05),
  labs = c('Probability:', round(fw_probs, 2)),
  Gender = 'Female/Woman'
)

anx_fit2_latent_normals |>
  filter(Gender == 'Female/Woman') |>
  ggplot(aes(x = x, y = d)) +
  geom_area(position = 'identity', alpha = 0.5, fill = palette_gender[1]) +
  geom_vline(xintercept = anx_fit2$zeta, linetype = 'dotted') +
  geom_text(data = fw_prob_labs, aes(label = labs), family = 'Fira Sans') +
  geom_text(data = rating_labs, aes(y = d-0.15, label = labs), family = 'Fira Sans', size = 8) +
  theme(
    axis.line.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    legend.position = 'none',
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(
    'Anxiety (assumed latent variable)', 
    breaks = -3:3,
    sec.axis = dup_axis(
      name = NULL,
      breaks = anx_fit2$zeta |> as.double(),
      labels = round(anx_fit2$zeta, 2)
    )) +
  ggtitle('Female/Woman') +
  NULL
```

```{r}
mm_probs <- get_prob_dens(anx_fit2$zeta, mm_mean, 1)

mm_prob_labs <- tibble(
  x = c(-2.6, -1.4, -0.56, 0.05, 0.66, 1.3),
  d = c(0.05),
  labs = c('Probability:', round(mm_probs, 2)),
  Gender = 'Male/Man'
)

anx_fit2_latent_normals |>
  filter(Gender == 'Male/Man') |>
  ggplot(aes(x = x, y = d)) +
  geom_area(position = 'identity', alpha = 0.5, fill = palette_gender[2]) +
  geom_vline(xintercept = anx_fit2$zeta, linetype = 'dotted') +
  geom_text(data = mm_prob_labs, aes(label = labs), family = 'Fira Sans') +
  geom_text(data = rating_labs, aes(y = d-0.15, label = labs), family = 'Fira Sans', size = 8) +
  theme(
    axis.line.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    legend.position = 'none',
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(
    'Anxiety (assumed latent variable)', 
    breaks = -3:3,
    sec.axis = dup_axis(
      name = NULL,
      breaks = anx_fit2$zeta |> as.double(),
      labels = round(anx_fit2$zeta, 2)
    )) +
  ggtitle('Male/Man') +
  NULL
```

```{r}
ag_probs <- get_prob_dens(anx_fit2$zeta, ag_mean, 1)

ag_prob_labs <- tibble(
  x = c(-2.6, -1.2, -0.56, 0.05, 0.66, 1.7),
  d = c(0.05),
  labs = c('Probability:', round(ag_probs, 2)),
  Gender = 'Another Gender'
)

anx_fit2_latent_normals |>
  filter(Gender == 'Another Gender') |>
  ggplot(aes(x = x, y = d)) +
  geom_area(position = 'identity', alpha = 0.5, fill = palette_gender[3]) +
  geom_vline(xintercept = anx_fit2$zeta, linetype = 'dotted') +
  geom_text(data = ag_prob_labs, aes(label = labs), family = 'Fira Sans') +
  geom_text(data = rating_labs, aes(y = d-0.15, label = labs), family = 'Fira Sans', size = 8) +
  theme(
    axis.line.x.top = element_blank(),
    axis.ticks.x.top = element_blank(),
    legend.position = 'none',
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(
    'Anxiety (assumed latent variable)', 
    breaks = -3:3,
    sec.axis = dup_axis(
      name = NULL,
      breaks = anx_fit2$zeta |> as.double(),
      labels = round(anx_fit2$zeta, 2)
    )) +
  ggtitle('Another Gender') +
  NULL
```

### Significance tests

`polr()` doesn't provide *p*-values:

```{r}
summary(anx_fit2)
```

But we do have *t*-values.
So to know whether we can reject the null hypothesis (H0) that there's no association between ratings and gender, we can just treat the estimated *t*-values like z-scores and compare them to a standard normal distribution representing the H0.^[
Ideally we'd be comparing *t*-values to a *t* distribution, but with increasing degrees of freedom, the *t* distribution better approximates the normal distribution.
And I'm not too worried about having too few degrees of freedom for a decent approximation, because `anx` contains a lot of observations.
]

```{r}
pnorm(abs(-10.880), lower.tail = FALSE) * 2
pnorm(abs(4.041), lower.tail = FALSE) * 2
```

In both cases, the probability of observing a value this extreme or more extreme, given the H0 of no effect given by the standard normal distribution, is less than 0.05.
So we can reject the null hypotheses that

- there's no difference between `Female/Woman` and `Male/Man` ratings, and
- there's no difference between `Female/Woman` and `Another Gender` ratings.


# Gaussian regression model with `lm()`

The most common (but least adequate?) way of analysing rating data is to treat it as numeric and use a Gaussian model like `lm()`.

```{r}
anx <- anx |>
  mutate(rating_num = as.numeric(rating))
```


Plotting this discrete data as if were continuous looks a bit strangeâ€”a hint that things aren't quite right.

```{r}
p_rating_num <- anx |>
  ggplot(aes(x = gender, y = rating_num, fill = gender, colour = gender)) +
  geom_violin(adjust = 3, alpha = 0.2) +
  geom_jitter(alpha = 0.2) +
  scale_fill_manual(values = palette_gender) +
  scale_colour_manual(values = palette_gender) +
  theme(legend.position = 'none') +
  labs(
    x = element_blank(),
    y = 'Rating (treated incorrectly as numeric)'
  ) +
  NULL

p_rating_num
```

But nevertheless, the model is fit:

```{r}
anx_lm <- lm(rating_num ~ gender, data = anx)

summary(anx_lm)
```

Interpretation:

- The `(Intercept)` represents the mean of the ratings for the reference level, `Female/Woman`.
- The parameter `genderMale/Man` gives the difference between the mean of the ratings for `Female/Woman` and the mean of the ratings for `Male/Man`.
- The parameter `genderAnother Gender` gives the difference between the mean of the ratings for `Female/Woman` and the mean of the ratings for `Another Gender`.

Here's a visualisation of what those parameters are doing, superimposed on the plot from above.

```{r}
p_rating_num +
  stat_summary(colour = 'black', fun = mean, geom = 'point', size = 2) +
  stat_summary(colour = 'black', fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  # line from fw to mm
  geom_segment(colour = 'black', aes(
    x = 1,
    xend = 2,
    y = coef(anx_lm)[['(Intercept)']],
    yend = coef(anx_lm)[['(Intercept)']] + coef(anx_lm)[['genderMale/Man']]
  )) +
  # line from fw to ag
  geom_segment(colour = 'black', aes(
    x = 1,
    xend = 3,
    y = coef(anx_lm)[['(Intercept)']],
    yend = coef(anx_lm)[['(Intercept)']] + coef(anx_lm)[['genderAnother Gender']]
  )) +
  NULL
```

Weird stuff!


# z-score conversion then Gaussian regression model with `lm()`

Another method of analysing rating data, more sophisticated than the last way but still assuming that rating data can be handled as if it were continuous numeric, first converts ratings to z-scores and then models those z-scores with a Gaussian model.

Converting ratings to z-scores by participant irons out individual participants' biases in how they use the scale.
The z-scores effectively compare each participant's responses to their own average.
So, this is a good way of accounting for (for example) whether people in particular gender groups tend to be more or less anxious on the whole.

However, it's this step where the funny business lies.
The process of z-scoring assumes that the variable to be transformed can sensibly be summarised in terms of its mean and standard deviation, assumptions that don't necessarily hold for discrete rating data.

Once the values are in z-score form, though, they are continuous numeric and can be reasonably modelled by a Gaussian model.

Anyway: to compute these z-scores, we need to know how participants responded to all the survey questions, not just the one being analysed here.
So I'll read in `attn.csv`, which contains all the responses from all the participants who passed all seven attention checks.

```{r}
attn <- read_csv('data/attn.csv')

# Do some wrangling, then group by participant and compute z-scores.
attn_z <- attn |> 
  select(unique_id, gender, starts_with('Q')) |> 
  pivot_longer(
    cols = starts_with('Q'), 
    names_to = 'q', 
    values_to = 'score', 
    values_transform = list(score = as.numeric)
  ) |>
  filter(!is.na(score), !is.na(gender)) |> 
  group_by(unique_id) |> 
  mutate(by_ppt_z = scale(score))  # get z-score with scale()

# Filter down to the question we care about.
anx_z <- attn_z |> 
  filter(q == 'Q7.1_3') |> 
  select(unique_id, gender, by_ppt_z) |> 
  mutate(gender = factor(gender, levels = c('Female/Woman', 'Male/Man', 'Another Gender')))

head(anx_z)
```

```{r echo=F}
rm(attn_z)
rm(attn)
```

Have a look at the distribution of z-scores for each gender group:

```{r}
p_rating_z <- anx_z |>
  ggplot(aes(x = gender, y = by_ppt_z, fill = gender, colour = gender)) +
  geom_violin(adjust = 3, alpha = 0.2) +
  geom_jitter(alpha = 0.1) +
  scale_fill_manual(values = palette_gender) +
  scale_colour_manual(values = palette_gender) +
  theme(legend.position = 'none') +
  labs(
    x = element_blank(),
    y = 'z-score (normalised by-participant)'
  ) +
  NULL

p_rating_z
```


Fit the model which predicts z-scores by gender group:

```{r}
anx_z_lm <- lm(by_ppt_z ~ gender, data = anx_z)
summary(anx_z_lm)
```

Interpretation:

- The `(Intercept)` represents the mean of the z-scores for the reference level, `Female/Woman`.
- The parameter `genderMale/Man` gives the difference between the mean of the z-scores for `Female/Woman` and the mean of the z-scores for `Male/Man`.
- The parameter `genderAnother Gender` gives the difference between the mean of the z-scores for `Female/Woman` and the mean of the z-scores for `Another Gender`.

Overlay the estimated lines on the plot from above:

```{r}
p_rating_z +
  stat_summary(fun = mean, geom = 'point', size = 2, colour = 'black') +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, colour = 'black') +
  # line from fw to mm
  geom_segment(colour = 'black', aes(
    x = 1,
    xend = 2,
    y = coef(anx_z_lm)[['(Intercept)']],
    yend = coef(anx_z_lm)[['(Intercept)']] + coef(anx_z_lm)[['genderMale/Man']]
  )) +
  # line from fw to ag
  geom_segment(colour = 'black', aes(
    x = 1,
    xend = 3,
    y = coef(anx_z_lm)[['(Intercept)']],
    yend = coef(anx_z_lm)[['(Intercept)']] + coef(anx_z_lm)[['genderAnother Gender']]
  )) +
  NULL
```


# Resources

- Jamieson's (2004) paper [Likert scales: How to (ab)use them.](https://onlinelibrary.wiley.com/doi/10.1111/j.1365-2929.2004.02012.x)

- UCLA Statistical Methods and Data Analytics's web page [Ordinal Logistic Regression.](https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/)

- Kurz' (2021) blog post [Notes on the Bayesian cumulative probit.](https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/)

- Gelman and Hill's (2007) book [Data Analysis Using Regression and Multilevel/Hierarchical Models.](https://www.cambridge.org/highereducation/books/data-analysis-using-regression-and-multilevel-hierarchical-models/32A29531C7FD730C3A68951A17C9D983)


# Session info

```{r}
sessionInfo()
```

